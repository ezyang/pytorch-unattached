{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transferring a model from PyTorch into Caffe2 using ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we describe how to use ONNX to convert a model defined in PyTorch into the ONNX format and then load it into Caffe2. Once in Caffe2 we can run the model to double check it was exported correctly, and then use Caffe2 features such as exporting the model for execution on mobile devices.\n",
    "\n",
    "This work is a joint collaboration among various awesome developers on PyTorch and Caffe2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import a bunch of stuff we are going to need for this\n",
    "import io\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import torch.onnx\n",
    "\n",
    "import onnx\n",
    "from onnx.backend import c2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we will transfer a super resolution model that can increase the resolution of photographs. First, let's create a SuperResolution model in PyTorch. [This model](https://github.com/pytorch/examples/blob/master/super_resolution/model.py) comes directly from PyTorch's examples without modification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Super Resolution model definition in PyTorch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "\n",
    "class SuperResolutionNet(nn.Module):\n",
    "    def __init__(self, upscale_factor, inplace=False):\n",
    "        super(SuperResolutionNet, self).__init__()\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=inplace)\n",
    "        self.conv1 = nn.Conv2d(1, 64, (5, 5), (1, 1), (2, 2))\n",
    "        self.conv2 = nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1))\n",
    "        self.conv3 = nn.Conv2d(64, 32, (3, 3), (1, 1), (1, 1))\n",
    "        self.conv4 = nn.Conv2d(32, upscale_factor ** 2, (3, 3), (1, 1), (1, 1))\n",
    "        self.pixel_shuffle = nn.PixelShuffle(upscale_factor)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.pixel_shuffle(self.conv4(x))\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        init.orthogonal(self.conv1.weight, init.calculate_gain('relu'))\n",
    "        init.orthogonal(self.conv2.weight, init.calculate_gain('relu'))\n",
    "        init.orthogonal(self.conv3.weight, init.calculate_gain('relu'))\n",
    "        init.orthogonal(self.conv4.weight)\n",
    "\n",
    "# Create the super-resolution model by using the above model definition.\n",
    "py_model = SuperResolutionNet(upscale_factor=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next download some pre-trained weights for this model to avoid having to train a model for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SuperResolutionNet (\n",
       "  (relu): ReLU ()\n",
       "  (conv1): Conv2d(1, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4): Conv2d(32, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pixel_shuffle): PixelShuffle (upscale_factor=3)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define input, load pretrained model weights\n",
    "model_url = 'https://s3.amazonaws.com/pytorch/test_data/export/superres_epoch100-44c6958e.pth'\n",
    "batch_size = 2    # just a random number\n",
    "\n",
    "# Initialize model with the pretrained weights\n",
    "py_model.load_state_dict(model_zoo.load_url(model_url))\n",
    "\n",
    "# set the train mode to false since we will only run the forward pass.\n",
    "py_model.train(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exporting a model in PyTorch works via tracing. To export a model, you call the `torch.onnx.export()` function which executes the model, recording a trace of what operators are used to compute the outputs. Because `export` runs the model, we need provide an input tensor `x`. The values in this tensor are not important, but it needs to be the right size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input to the model\n",
    "x = Variable(torch.randn(batch_size, 1, 224, 224), requires_grad=True)\n",
    "\n",
    "# Export the model \n",
    "torch_out = torch.onnx.export(py_model, # model being run \n",
    "                              x, # the input to the model (or a tuple for multiple inputs) \n",
    "                              \"super_resolution.onnx\", # where to save the model (can also be a file or file-like object)\n",
    "                              export_params=True) # store the trained parameter weights inside the model file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch_out` is the output after executing the model. Normally you can ignore this output, but here we will use it to verify that the model we exported computes the same values when run in Caffe2.\n",
    "\n",
    "\n",
    "Now let's take this ONNX model and use it in Caffe2. This part can normally be done in seperate process or on another machine, but we will continue in the same process so that we can verify that Caffe2 and PyTorch are computing the same value for the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ONNX GraphProto object.\n",
    "# graph is a standard Python protobuf object\n",
    "graph = onnx.load(\"super_resolution.onnx\")\n",
    "\n",
    "# prepare the caffe2 backend for executing the model\n",
    "# this converts the ONNX graph into a Caffe2 NetDef \n",
    "# that can execute it\n",
    "prepared = c2.prepare(graph)\n",
    "\n",
    "# run the model in Caffe2\n",
    "\n",
    "# Construct a map from input names to Tensor data.\n",
    "# The graph itself contains inputs for all weight parameters, followed by the input image.\n",
    "# Since the weights are already embedded, we just need to pass the input image, which is the \n",
    "# last input the graph\n",
    "W = {graph.input[-1]: x.data.numpy()}\n",
    "\n",
    "# Run the Caffe2 net:\n",
    "c2_out = prepared.run(W)[0]\n",
    "\n",
    "# Verify the numerical correctness upto 3 decimal places\n",
    "np.testing.assert_almost_equal(torch_out.data.cpu().numpy(), c2_out, decimal=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the model on mobile devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have exported a model from PyTorch and shown how to load it and run it in Caffe2. Now that the model is loaded in Caffe2, we can convert it into a format suitable for [running on mobile devices](https://caffe2.ai/docs/mobile-integration.html).\n",
    "\n",
    "We will use Caffe2's [mobile_exporter](https://github.com/caffe2/caffe2/blob/master/caffe2/python/predictor/mobile_exporter.py) to generate the two model protobufs that can run on mobile. The first is used to initialize the network, and the second actual runs a forward inference step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract the workspace and the graph proto from the internal representation\n",
    "c2_workspace = prepared.workspace\n",
    "c2_graph = prepared.predict_net\n",
    "\n",
    "# Now import the caffe2 mobile exporter\n",
    "from caffe2.python.predictor import mobile_exporter\n",
    "\n",
    "# call the Export to get the predict_net, init_net\n",
    "# init_net, predict_net = mobile_exporter.Export(c2_workspace, c2_graph, c2_graph.input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now, on your ios/Android device, you can use the above protobufs and use caffe2::Predictor (iOS) or Caffe2 instance (Android) for deploying them real-time. For more information, also checkout caffe2 https://caffe2.ai/docs/AI-Camera-demo-android.html</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
