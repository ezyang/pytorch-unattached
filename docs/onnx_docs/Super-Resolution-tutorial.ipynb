{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Super-Resolution e2e model from Pytorch -> Mobile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> In this tutorial, we describe how to use ONNX to convert a well-defined SuperResolution Pytorch model to some intermediate representation (IR) and use this IR to run the model in Caffe2 backend and also on Mobile. </p>\n",
    "<br>\n",
    "<p> This work is a joint collaboration among various awesome developers on Pytorch and Caffe2 </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 1: Let's import a bunch of stuff we are going to need for this\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import io\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import torch.onnx.export as export_model\n",
    "\n",
    "import onnx\n",
    "import onnx.backend.c2 as backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now, let's create a SuperResolution model in Pytorch. This model definition can be found at https://github.com/pytorch/examples/blob/master/super_resolution/model.py</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Super Resolution model definition in PyTorch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "\n",
    "class SuperResolutionNet(nn.Module):\n",
    "    def __init__(self, upscale_factor, inplace=False):\n",
    "        super(SuperResolutionNet, self).__init__()\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=inplace)\n",
    "        self.conv1 = nn.Conv2d(1, 64, (5, 5), (1, 1), (2, 2))\n",
    "        self.conv2 = nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1))\n",
    "        self.conv3 = nn.Conv2d(64, 32, (3, 3), (1, 1), (1, 1))\n",
    "        self.conv4 = nn.Conv2d(32, upscale_factor ** 2, (3, 3), (1, 1), (1, 1))\n",
    "        self.pixel_shuffle = nn.PixelShuffle(upscale_factor)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.pixel_shuffle(self.conv4(x))\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        init.orthogonal(self.conv1.weight, init.calculate_gain('relu'))\n",
    "        init.orthogonal(self.conv2.weight, init.calculate_gain('relu'))\n",
    "        init.orthogonal(self.conv3.weight, init.calculate_gain('relu'))\n",
    "        init.orthogonal(self.conv4.weight)\n",
    "\n",
    "# Step 2: Create the super-resolution model by using the above model definition.\n",
    "py_model = SuperResolutionNet(upscale_factor=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>NOTE:</b> The above model definition has inplace=False since ONNX doesn't supprt inplace operations yet but will soon. Please watch changelog {INSERT CHANGELOG doc link here}</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> For the purpose of this tutorial, we will use a pre-trained super-resolution model. Note that this is optional.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 3: define input, load pretrained model weights\n",
    "model_url = 'https://s3.amazonaws.com/pytorch/test_data/export/superres_epoch100-44c6958e.pth'\n",
    "batch_size = 2    # just a random number\n",
    "\n",
    "# Input to the model\n",
    "x = Variable(torch.randn(batch_size, 1, 224, 224), requires_grad=True)\n",
    "\n",
    "# Initialize model with the pretrained weights\n",
    "py_model.load_state_dict(model_zoo.load_url(model_url))\n",
    "\n",
    "# set the train mode to false since we will only run the forward pass.\n",
    "py_model.train(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now, we will export the model to a serialized ONNX IR proto which is readable/executable by other backends like Caffe2, CNTK etc. In the steps below, we'll see how to do the export, run the model in Caffe2 backend and verify the numerical correctness.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 4: convert the model to ONNX, run in Caffe2 and verify numerical correctness\n",
    "\n",
    "# first, create a file-like object where the serialized IR will be written\n",
    "onnx_ir = io.BytesIO()\n",
    "\n",
    "# Now, export the model to ONNX and run the model in PyTorch\n",
    "torch_out = export_model(model, x, onnx_ir, export_params=True)\n",
    "\n",
    "# Load the IR in ONNX to get the graph protobuf\n",
    "graph = onnx.load(onnx_ir)\n",
    "\n",
    "# prepare the caffe2 backend for executing the model\n",
    "internal_rep = backend.prepare(graph)\n",
    "\n",
    "# run the model in caffe2 backend\n",
    "c2_out = backend.run(internal_rep, x)[0]\n",
    "\n",
    "# Verify the numerical correctness upto 3 decimal places\n",
    "np.testing.assert_almost_equal(torch_out.data.cpu().numpy(), c2_out, decimal=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>NOTE:</b> <i>export_params=True</i> allows passing the model parameters with the serialized ONNX IR proto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Using very simple steps above, we see how easy it is to convert a PyTorch model to an IR consumable by other backends and run the model there. Now, in next few steps, we'll see how to convert the model so that it can be run on mobile.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the model on mobile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Caffe2 supports running networks on mobile very easily [information here: https://caffe2.ai/docs/mobile-integration.html]. \n",
    "We will use the Caffe2 mobile_exporter [https://github.com/caffe2/caffe2/blob/master/caffe2/python/predictor/mobile_exporter.py] to generate the two model protobufs that can run on mobile.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract the workspace and the graph proto from the internal representation\n",
    "c2_workspace = internal_rep.workspace\n",
    "c2_graph = internal_rep.predict_net\n",
    "\n",
    "# Now import the caffe2 mobile exporter\n",
    "from caffe2.python.predictor import mobile_exporter\n",
    "\n",
    "# call the Export to get the predict_net, init_net\n",
    "init_net, predict_net = mobile_exporter.Export(c2_workspace, c2_graph, c2_graph.input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now, on your ios/Android device, you can use the above protobufs and use caffe2::Predictor (iOS) or Caffe2 instance (Android) for deploying them real-time. For more information, also checkout caffe2 https://caffe2.ai/docs/AI-Camera-demo-android.html</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
